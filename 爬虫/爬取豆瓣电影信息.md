```python
import csv
import time
import requests #基于urllib的请求库
import random
import json


path=r"C:\Users\Zhaowei\Desktop\暑假实训\data.csv"
headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36'}


def write_into_csv(path,items):
    with open(path,'w',encoding="utf-8",newline="")as fp:
        writer = csv.writer(fp)
        writer.writerows(items)


def read_from_csv(path):
    items=[]
    with open(path,'r',encoding="utf-8",newline="")as csvfile:
        reader=csv.reader(csvfile)
        for line in reader:
            items.append(line)
    return items

if __name__=="__main__":
    url='https://movie.douban.com/j/search_subjects?'
    # 完整url为 https://movie.douban.com/j/search_subjects?type=movie&tag=%E7%83%AD%E9%97%A8&sort=recommend&page_limit=20&page_start=0

    items=[]
    for i in range(3): #60条
        params={
            'type':'movie',
            'tag':'热门',
            'sort':'recommend',
            'page_limit':'20',
            'page_start':str(i*20) #起始编号为i*20，每页20部电影
        }
        res=requests.get(url,headers=headers,params=params)
        time.sleep(random.randint(1,5))
        data=json.loads(res.text)['subjects'] #json解析页面，整个页面内容在"subjects"中
        for movie in data:
            movie_name=movie['title']
            movie_rate=movie['rate']
            movie_url=movie['url']
            item=[movie_name,movie_rate,movie_url]
            items.append(item)
        with open(path,'w',encoding="utf-8",newline="")as csvfp:
            write=csv.writer(csvfp)
            with open(path,'r',encoding="utf-8",newline="")as fp:
                read=csv.reader(fp)
                if not [row for row in read]:
                    write.writerow(['电影名','评分','地址'])
                    write.writerows(items)
                else:
                    write.writerows(items)


```
```python
import csv
import time
import random
import json
import requests #基于urllib的请求库
from bs4 import BeautifulSoup
path2=r"C:\Users\Zhaowei\Desktop\暑假实训\genre.csv"
path=r"C:\Users\Zhaowei\Desktop\暑假实训\data.csv"
headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36'}


def write_into_csv(path,items,attrs):
    with open(path,'w',encoding="utf-8",newline="")as fp:
        writer = csv.writer(fp)
        writer.writerow(attrs) #属性列名
        writer.writerows(items)


def read_from_csv(path):
    items=[]
    with open(path,'r',encoding="utf-8",newline="")as csvfile:
        reader=csv.reader(csvfile)
        for line in reader:
            items.append(line)
    return items

if __name__=="__main__":
    cnt=0
    movie_list=read_from_csv(path)[1:]
    movie_info=[]
    for movie in movie_list:
        cnt+=1
        url=movie[2]
        r=requests.Session()
        r=BeautifulSoup(r.get(url,headers=headers).content,features="lxml")
        time.sleep(random.randint(1,5))
        all=r.find_all('span',{"property":'v:genre'}) # <span property="v:genre"> 体裁
        movie_genre=''
        for each in all:
            movie_genre+=str(each.text)+' '
        movie_info.append([movie[0],movie_genre])
        print("第{}项任务已完成...".format(cnt))
    write_into_csv(path2,movie_info,['电影名','体裁'])
```
