```
LDA主题模型：3层贝叶斯概率模型，包含文档（d）、主题（z）和词（w）3层结构。能够挖掘数据集中的潜在主题，进而分析数据集的集中关注点及其相关特征词。

狄利克雷分布

先验分布：与试验结果无关，或与随机抽样无关，反映在进行统计试验之前根据其他有关参数口的知识而得到的分布
后验分布：根据样本 X 的分布Pθ及θ的先验分布π(θ)，用概率论中求条件概率分布的方法

狄利克雷分布模型LDA：模型假设话题由单词的多项分布表示，文本由话题的多项分布表示，单词分布和话题分布的先验分布都是狄利克雷分布。话题在文本中服从狄利克雷分布，单词在话题中也服从狄利克雷分布。

LDA吉布斯抽样算法
LDA变分EM算法：通过解析的方法计算模型的后验概率p(z|x)的近似值


```

LDA：基于词频

```python
import pandas as pd
stoplist = r'C:\Users\Zhaowei\Desktop\暑假实训\去重\0709\stoplist.txt'	# 停用词

neg = pd.read_csv(r'C:\Users\Zhaowei\Desktop\暑假实训\去重\0709\neg.txt',encoding="utf-8",header=None,delimiter='\n')
pos = pd.read_csv(r'C:\Users\Zhaowei\Desktop\暑假实训\去重\0709\pos.txt',encoding="utf-8",header=None,delimiter='\n')
# 注意：导入txt文件选择编码之前，要在txt的另存为中选择utf-8编码方式覆盖原文件，不然会报错

# sep设置分割词，由于csv默认以半角逗号为分割词，而该词恰好在停词表中，因此，会导致读取错误
# 所以解决办法是手动设置一个不存在的分割词，如"tipdm",因为sep设置为多字符字符串，所以需要设置engine
# engine取值为python或者c，c的速度快但是python更全面，一般取python
stop = pd.read_csv(stoplist,encoding="utf-8",header=None,sep="tipdm",engine="python")
# 停用词表转换为列表，加上空格和空字符串
stop = [" ",""]+list(stop[0])

# 将每一个评论中分词，转换为一个列表的列表项，通过空格分隔
neg[1] = neg[0].apply(lambda s:s.split(" "))
neg[2] = neg[1].apply(lambda x:[i for i in x if i not in stop])

pos[1] = pos[0].apply(lambda s:s.split(" "))
pos[2] = pos[1].apply(lambda x:[i for i in x if i not in stop])


#导入LDA主题分析的人工智能模块
from gensim import corpora,models

#制作词典:词语的向量化
neg_dict = corpora.Dictionary(neg[2])

#制作语料：将给给定语料（词典）转换为词袋模型
neg_corpus = [neg_dict.doc2bow(i) for i in neg[2]]

#LDA模型训练
neg_lda = models.LdaModel(neg_corpus,num_topics=3,id2word=neg_dict) # 差评特征

#打印输出主题
print(neg_lda.print_topics(num_topics=3))
# f = open("./WEEK 02/data/neg.txt","w",encoding="utf-8")
# for i in range(3):
#   f.write(neg_lda.print_topic(i))
#   f.write("\n")


pos_dict = corpora.Dictionary(pos[2])
pos_corpus = [pos_dict.doc2bow(i) for i in pos[2]]
pos_lda = models.LdaModel(pos_corpus,num_topics=3,id2word=pos_dict) # 好评特征

#打印输出主题
print(neg_lda.print_topics(num_topics=3))
# f = open("./WEEK 02/data/pos.txt","w",encoding="utf-8")
# for i in range(3):
#   f.write(pos_lda.print_topic(i))
#   f.write("\n")

output:
[(0, '0.001*"the" + 0.001*"in" + 0.001*"机器和价格都不错" + 0.001*"4999买的" + 0.001*"not" + 0.001*"I" + 0.000*"home" + 0.000*"设计非常漂亮,散热很棒" + 0.000*"小巧" + 0.000*"东西还不错"'), (1, '0.002*"很满意" + 0.001*"耐心解答。全五分" + 0.001*"继续努力" + 0.001*"就是安装太贵了" + 0.001*"对美的售后不满" + 0.001*"对本店售后很满意" + 0.001*"对产品很满意" + 0.001*"不错" + 0.001*"每次及时回复" + 0.001*"要260"'), (2, '0.004*"回复" + 0.003*"转发微博" + 0.002*"抵制蒙牛" + 0.002*"1." + 0.002*"(来自" + 0.002*"（分享自" + 0.001*"汤臣倍健不买了" + 0.001*"步步高点读机不买了" + 0.001*"爱透不买了" + 0.001*"乜原因啊"')]
[(0, '0.001*"the" + 0.001*"in" + 0.001*"机器和价格都不错" + 0.001*"4999买的" + 0.001*"not" + 0.001*"I" + 0.000*"home" + 0.000*"设计非常漂亮,散热很棒" + 0.000*"小巧" + 0.000*"东西还不错"'), (1, '0.002*"很满意" + 0.001*"耐心解答。全五分" + 0.001*"继续努力" + 0.001*"就是安装太贵了" + 0.001*"对美的售后不满" + 0.001*"对本店售后很满意" + 0.001*"对产品很满意" + 0.001*"不错" + 0.001*"每次及时回复" + 0.001*"要260"'), (2, '0.004*"回复" + 0.003*"转发微博" + 0.002*"抵制蒙牛" + 0.002*"1." + 0.002*"(来自" + 0.002*"（分享自" + 0.001*"汤臣倍健不买了" + 0.001*"步步高点读机不买了" + 0.001*"爱透不买了" + 0.001*"乜原因啊"')]
```

```python
from sklearn.decomposition import LatentDirichletAllocation as LDA
from sklearn.feature_extraction.text import CountVectorizer
stoplist=[]
# with open('r')as fp:
#     doc_neg=fp.read()
with open(r'C:\Users\Zhaowei\Desktop\暑假实训\去重\0709\cuted_words.txt','r')as fp:
    doc_pos=fp.read().replace('/',' ')
with open(r'C:\Users\Zhaowei\Desktop\暑假实训\去重\0709\stoplist.txt','r',encoding='utf-8')as fp:
    lines=fp.readlines()
    for line in lines:
        stoplist.extend(line.strip().strip('\n'))

corpus=[doc_neg,doc_pos]
cntVector=CountVectorizer(stop_words=stoplist)
cntTF=cntVector.fit_transform(corpus)   # 词频统计
feature_name=cntVector.get_feature_names()
print(cntTF)
lda=LDA(n_components=3)
docres=lda.fit_transform(cntTF)
print('docres',docres)
print(lda.components_)

def print_top_words(model,feature_name,n_stop_words):
    for topic_id,topic in enumerate(model.components_):
        print('topic_id{} '.format(topic_id))
        print(' '.join([feature_name[i] for i in topic.argsort()[:-n_stop_words-1:-1]]))

print_top_words(lda,feature_name,20)

# 各topic中的词频
print('归一化：')
for i in range(len(lda.components_)):
    lda.components_[i]=[x/lda.components_[i].sum() for x in lda.components_[i]]
print(lda.components_)
```