语义关系

构建语义关联矩阵

```python
import re  # 正则表达式库
import jieba  # 分词
import collections  # 词频统计库
import numpy as np
import pandas as pd
import networkx as nx  # 复杂网络分析库
import matplotlib.pyplot as plt


num = 30  # 最高频的num个词
G = nx.Graph()
plt.figure(figsize=(20, 14))
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签

# 读取文件
fn = open(r'C:\Users\Zhaowei\Desktop\暑假实训\去重\0708\comment_piece.txt','r', encoding='utf-8')  # 打开文件
string_data = fn.read()  # 读出整个文件
fn.close()  # 关闭文件

# 文本预处理
pattern = re.compile(u'\t|\。|，|：|；|！|\）|\（|\?|"')  # 定义正则表达式匹配模式
string_data = re.sub(pattern, '', string_data)  # 将符合模式的字符去除
print(string_data)

# 文本分词
seg_list_exact = jieba.cut(string_data, cut_all=False)  # 对整个文本精确模式分词
remove_words = list(open(r'C:\Users\Zhaowei\Desktop\暑假实训\去重\0708\stop_words.txt', 'r', encoding='utf-8').read())  # 自定义去除词库
remove_words.append("\n")

object_list = []    # 所有需要计数的词
for word in seg_list_exact:  # 循环读出每个分词
    if word not in remove_words:  # 如果不在去除词库中
        object_list.append(word)  # 分词追加到列表
print('object_list的元素个数：', len(object_list))
print(object_list)

# 词频统计
word_counts = collections.Counter(object_list)  # 对分词做词频统计
word_counts_top = word_counts.most_common(num)  # 获取最高频的num个词
word = pd.DataFrame(word_counts_top, columns=['关键词', '次数'])
print(word)

word_T = pd.DataFrame(word.values.T, columns=word.iloc[:, 0])   # 转置
word_T.to_excel(r'C:\Users\Zhaowei\Desktop\暑假实训\去重\0708\word_counts.xls')
print('initialized_word_T:')
print(word_T)
net = pd.DataFrame(np.mat(np.zeros((num, num))), columns=word.iloc[:, 0])   # 形成关联矩阵np.mat
print('initialized_net:')
print(net)


# 构建语义关联矩阵，列为词语，行为
object_list2 = []
k=0 # k是每行的头部指针，每次遇到换行就更新
for i in range(len(string_data)):
    if string_data[i] == '\n':  # 根据换行符读取一行文字
        seg_list_exact = jieba.cut(string_data[k:i], cut_all=False)  # 精确模式分词
        k=i+1
        seg_list_exact=list(seg_list_exact)  # ！！！
        for words in seg_list_exact:  # 循环读出每个分词
            if words not in remove_words:  # 如果不在去除词库中
                object_list2.append(words)  # 分词追加到列表
print(len(object_list2))
print('object_list2',object_list2)

word_counts2 = collections.Counter(object_list2)
word_counts_top2 = word_counts2.most_common(num)  # 获取该段最高频的num个词
word2 = pd.DataFrame(word_counts_top2)
word2_T = pd.DataFrame(word2.values.T, columns=word2.iloc[:, 0])
word2_T.to_excel(r'C:\Users\Zhaowei\Desktop\暑假实训\去重\0708\word2_T.xls')
print('word2_T:')
print(word2_T)
relation = list(0 for x in range(num))
# 查看该段最高频的词是否在总的最高频的词列表word2中
for j in range(num):
    for p in range(len(word2)):
        if word.iloc[j, 0] == word2.iloc[p, 0]: # word[j][0]==word2[p][0]
            relation[j] = 1
            break
        # 对于同段落内出现的最高频词，根据其出现次数加到语义关联矩阵的相应位置
for j in range(num):
    if relation[j] == 1:
        for q in range(num):
            if relation[q] == 1:
                net.iloc[j, q] += word2_T.loc[1, word_T.iloc[0, q]]
print(net)
net.to_excel(r'C:\Users\Zhaowei\Desktop\暑假实训\去重\0708\net.xls')
# 处理最后一段内容，完成语义关联矩阵的构建


n = len(word)
for i in range(n):
    for j in range(i, n):
        # 加入边(起点，终点，权重)
        G.add_weighted_edges_from([(word.iloc[i, 0], word.iloc[j, 0], net.iloc[i, j])])
print(G.edges())
nx.draw_networkx(G,pos=nx.spring_layout(G),node_color='white',edge_color='blue')
plt.axis('off')
# plt.show()


# word是所有词里选最高频的num个，word2是可以任取一段文本，统计最高频的num个，net矩阵是以word为基础构建的，在word2里的word标记为1
# 结果出来就是这一段文本的关联矩阵，矩阵的权就是它在这一段文本中的频数
# 上面的代码中从头到尾取了word2，因此word=word2
# 注1：word2是以换行为间隔取段落，如果原文本最后少一个换行，最后一句话就无法读入word2
# 注2：seg_list_exact=list(seg_list_exact)  # jieba.cut后的对象是generate类型，必须先转换成list才能访问其元素
```

### LSA(Latent semantic analysis)：潜在语义分析，解决一词多义

主题模型：分析文档的主体构成

m个文档，n个单词，k个主题，生成m*n的文档-词项矩阵A

SVD分解成三个矩阵：U（文档×主题），S（主题×主题），V^{T}（主题×单词）
$$
A=USV^{T}
$$

```python
import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD

# pd.set_option("display.max_colwidth", 200)
data = pd.read_csv(r'C:\Users\Zhaowei\Desktop\暑假实训\comment.csv')
print(data)

pattern = re.compile(u'\t|\。|，|：|；|;|,|！|\）|\（|\?|"')  # 定义正则表达式匹配模式
func = lambda x: re.sub(pattern, '', x)  # 将符合模式的字符去除
data['clean'] = data['评论内容'].apply(func)

vectorizer = TfidfVectorizer(max_features=1000, max_df=0.5, smooth_idf=True)
X = vectorizer.fit_transform(data['clean'])
print('X.shape:',X.shape)  # 生成矩阵的形状（行数=商品数，列数=属性数）

# SVD分解：n_components=特征数，严格小于输入矩阵中的列数
svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)
svd_model.fit(X)
print('len(svd_model.components_):',len(svd_model.components_))

terms = vectorizer.get_feature_names()

for i, comp in enumerate(svd_model.components_):
    terms_comp = zip(terms, comp)
    sorted_terms = sorted(terms_comp, key=lambda x: x[1], reverse=True)[:7]
    print("features " + str(i) + ": ")  # 每种特征打印关键词
    for t in sorted_terms:
        print(t[0])
        print(" ")
        
输入：文档
输出：特征
```

朴素贝叶斯：用于分类，即给一个多特征坐标，判断它是属于哪一类（当然不一定就属于这一类，只是概率较大）

A：类别，B：特征，P(A|B)：在该特征的条件下，类别为A的概率

B_i来自样本，B发生相当于所有B_i同时发生

“朴素”在于特征之间相互独立


$$
P(A|B)=\frac{P(B|A)*P(A)}{P(B)}=\frac{P(B_1|A)*P(B_2|A)*...*P(B_n|A)*P(A)}{P(B)}
$$

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

pos_path=r'C:\Users\Zhaowei\Desktop\暑假实训\去重\0708\朴素贝叶斯\merry.txt'
neg_path=r'C:\Users\Zhaowei\Desktop\暑假实训\去重\0708\朴素贝叶斯\merry_not.txt'
pos=pd.read_csv(pos_path,sep=',')
neg=pd.read_csv(neg_path,sep=',')
# sep是文本中数据的分隔符，如果是tab要写成sep='\t'
# header=None适用于文本中没有列名的情况，这样第一行是train_data[0]，若有列名就不写
pos['mark']=1
neg['mark']=-1
train_data=pd.concat([ps,ng],ignore_index=True)
x=train_data[['handsome','character','height','hard']]
y=train_data['mark']

X_train, X_test, y_train, y_test = train_test_split(train_data,train_target, test_size=0.25, random_state=33)
# shuffle=False 不随机分训练集和验证集，按索引顺序取后0.25为验证集

vec = CountVectorizer()  # 创建 特征值向量转化模块对象
X_train = vec.fit_transform(X_train)  # 标准化训练数据集
X_test = vec.transform(X_test)  # 标准化测试数据集
mnb = MultinomialNB()  # 使用朴素贝叶斯分类器对象
mnb.fit(X_train, y_train)  # 模型训练
predict = mnb.predict(X_test)  # 预测结果储存在变量predict中
print(predict)
```
